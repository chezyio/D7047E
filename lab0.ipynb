{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D7047E Advanced Deep Learning\n",
    "## Lab 0\n",
    "Refresher on PyTorch for course exercises and project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # NVIDIA GPU\n",
    "elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")   # Apple GPU (MPS)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")   # Fallback to CPU\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 — CNN uisng CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    ")\n",
    "\n",
    "train_set = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    "\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set, \n",
    "    batch_size=32, \n",
    "    shuffle=True,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "test_set = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_set, \n",
    "    batch_size=32, \n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 50000\n",
      "Test set size: 10000\n",
      "Batch shape: torch.Size([32, 3, 32, 32])\n",
      "Labels: tensor([3, 7, 6, 1, 4, 6, 2, 7, 5, 9, 4, 6, 7, 9, 0, 3, 0, 4, 5, 2, 1, 9, 5, 7,\n",
      "        4, 2, 1, 9, 4, 3, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training set size: {len(train_set)}\")\n",
    "print(f\"Test set size: {len(test_set)}\")\n",
    "\n",
    "# Example: Iterate through one batch to check\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "print(f\"Batch shape: {images.shape}\")  # Should be [batch_size, 3, 32, 32]\n",
    "print(f\"Labels: {labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, activation):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(32 * 8 * 8, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.activation = activation\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.pool(self.activation(self.conv1(x)))\n",
    "        x = self.pool(self.activation(self.conv2(x)))\n",
    "        x = x.view(-1, 32 * 8 * 8)\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.fc2(x)  # No activation on output\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model: nn.Module, train_loader: DataLoader, test_loader: DataLoader,\n",
    "                       optimizer: optim.Optimizer, criterion: nn.Module, device: torch.device,\n",
    "                       writer: SummaryWriter, experiment_name: str, num_epochs: int = 10) -> float:\n",
    "    print(f\"Starting {experiment_name}\")\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i, data in enumerate(train_loader):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            global_step = epoch * len(train_loader) + i\n",
    "            writer.add_scalar(f\"Loss/Train/{experiment_name}\", loss.item(), global_step)\n",
    "\n",
    "            if i % 200 == 199:\n",
    "                avg_loss = running_loss / 200\n",
    "                print(f\"[{experiment_name}] Epoch {epoch + 1}, Batch {i + 1}, Loss: {avg_loss:.3f}\")\n",
    "                running_loss = 0.0\n",
    "\n",
    "        train_accuracy = 100 * correct / total\n",
    "        writer.add_scalar(f\"Accuracy/Train/{experiment_name}\", train_accuracy, epoch)\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    test_accuracy = 100 * correct / total\n",
    "    writer.add_scalar(f\"Accuracy/Test/{experiment_name}\", test_accuracy, num_epochs)\n",
    "    print(f\"{experiment_name} - Test Accuracy: {test_accuracy:.2f}%\")\n",
    "    return test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Starting LeakyReLU_SGD\n",
      "[LeakyReLU_SGD] Epoch 1, Batch 200, Loss: 2.303\n",
      "[LeakyReLU_SGD] Epoch 1, Batch 400, Loss: 2.305\n",
      "[LeakyReLU_SGD] Epoch 1, Batch 600, Loss: 2.304\n",
      "[LeakyReLU_SGD] Epoch 1, Batch 800, Loss: 2.303\n",
      "[LeakyReLU_SGD] Epoch 1, Batch 1000, Loss: 2.302\n",
      "[LeakyReLU_SGD] Epoch 1, Batch 1200, Loss: 2.301\n",
      "[LeakyReLU_SGD] Epoch 1, Batch 1400, Loss: 2.303\n",
      "[LeakyReLU_SGD] Epoch 2, Batch 200, Loss: 2.302\n",
      "[LeakyReLU_SGD] Epoch 2, Batch 400, Loss: 2.301\n",
      "[LeakyReLU_SGD] Epoch 2, Batch 600, Loss: 2.302\n",
      "[LeakyReLU_SGD] Epoch 2, Batch 800, Loss: 2.302\n",
      "[LeakyReLU_SGD] Epoch 2, Batch 1000, Loss: 2.301\n",
      "[LeakyReLU_SGD] Epoch 2, Batch 1200, Loss: 2.300\n",
      "[LeakyReLU_SGD] Epoch 2, Batch 1400, Loss: 2.299\n",
      "[LeakyReLU_SGD] Epoch 3, Batch 200, Loss: 2.300\n",
      "[LeakyReLU_SGD] Epoch 3, Batch 400, Loss: 2.300\n",
      "[LeakyReLU_SGD] Epoch 3, Batch 600, Loss: 2.300\n",
      "[LeakyReLU_SGD] Epoch 3, Batch 800, Loss: 2.299\n",
      "[LeakyReLU_SGD] Epoch 3, Batch 1000, Loss: 2.298\n",
      "[LeakyReLU_SGD] Epoch 3, Batch 1200, Loss: 2.298\n",
      "[LeakyReLU_SGD] Epoch 3, Batch 1400, Loss: 2.298\n",
      "[LeakyReLU_SGD] Epoch 4, Batch 200, Loss: 2.298\n",
      "[LeakyReLU_SGD] Epoch 4, Batch 400, Loss: 2.298\n",
      "[LeakyReLU_SGD] Epoch 4, Batch 600, Loss: 2.297\n",
      "[LeakyReLU_SGD] Epoch 4, Batch 800, Loss: 2.296\n",
      "[LeakyReLU_SGD] Epoch 4, Batch 1000, Loss: 2.297\n",
      "[LeakyReLU_SGD] Epoch 4, Batch 1200, Loss: 2.297\n",
      "[LeakyReLU_SGD] Epoch 4, Batch 1400, Loss: 2.297\n",
      "[LeakyReLU_SGD] Epoch 5, Batch 200, Loss: 2.297\n",
      "[LeakyReLU_SGD] Epoch 5, Batch 400, Loss: 2.296\n",
      "[LeakyReLU_SGD] Epoch 5, Batch 600, Loss: 2.295\n",
      "[LeakyReLU_SGD] Epoch 5, Batch 800, Loss: 2.294\n",
      "[LeakyReLU_SGD] Epoch 5, Batch 1000, Loss: 2.295\n",
      "[LeakyReLU_SGD] Epoch 5, Batch 1200, Loss: 2.294\n",
      "[LeakyReLU_SGD] Epoch 5, Batch 1400, Loss: 2.294\n",
      "[LeakyReLU_SGD] Epoch 6, Batch 200, Loss: 2.294\n",
      "[LeakyReLU_SGD] Epoch 6, Batch 400, Loss: 2.294\n",
      "[LeakyReLU_SGD] Epoch 6, Batch 600, Loss: 2.294\n",
      "[LeakyReLU_SGD] Epoch 6, Batch 800, Loss: 2.293\n",
      "[LeakyReLU_SGD] Epoch 6, Batch 1000, Loss: 2.293\n",
      "[LeakyReLU_SGD] Epoch 6, Batch 1200, Loss: 2.292\n",
      "[LeakyReLU_SGD] Epoch 6, Batch 1400, Loss: 2.292\n",
      "[LeakyReLU_SGD] Epoch 7, Batch 200, Loss: 2.292\n",
      "[LeakyReLU_SGD] Epoch 7, Batch 400, Loss: 2.292\n",
      "[LeakyReLU_SGD] Epoch 7, Batch 600, Loss: 2.292\n",
      "[LeakyReLU_SGD] Epoch 7, Batch 800, Loss: 2.292\n",
      "[LeakyReLU_SGD] Epoch 7, Batch 1000, Loss: 2.288\n",
      "[LeakyReLU_SGD] Epoch 7, Batch 1200, Loss: 2.289\n",
      "[LeakyReLU_SGD] Epoch 7, Batch 1400, Loss: 2.290\n",
      "[LeakyReLU_SGD] Epoch 8, Batch 200, Loss: 2.289\n",
      "[LeakyReLU_SGD] Epoch 8, Batch 400, Loss: 2.289\n",
      "[LeakyReLU_SGD] Epoch 8, Batch 600, Loss: 2.288\n",
      "[LeakyReLU_SGD] Epoch 8, Batch 800, Loss: 2.288\n",
      "[LeakyReLU_SGD] Epoch 8, Batch 1000, Loss: 2.288\n",
      "[LeakyReLU_SGD] Epoch 8, Batch 1200, Loss: 2.289\n",
      "[LeakyReLU_SGD] Epoch 8, Batch 1400, Loss: 2.287\n",
      "[LeakyReLU_SGD] Epoch 9, Batch 200, Loss: 2.287\n",
      "[LeakyReLU_SGD] Epoch 9, Batch 400, Loss: 2.286\n",
      "[LeakyReLU_SGD] Epoch 9, Batch 600, Loss: 2.285\n",
      "[LeakyReLU_SGD] Epoch 9, Batch 800, Loss: 2.284\n",
      "[LeakyReLU_SGD] Epoch 9, Batch 1000, Loss: 2.286\n",
      "[LeakyReLU_SGD] Epoch 9, Batch 1200, Loss: 2.285\n",
      "[LeakyReLU_SGD] Epoch 9, Batch 1400, Loss: 2.284\n",
      "[LeakyReLU_SGD] Epoch 10, Batch 200, Loss: 2.283\n",
      "[LeakyReLU_SGD] Epoch 10, Batch 400, Loss: 2.283\n",
      "[LeakyReLU_SGD] Epoch 10, Batch 600, Loss: 2.283\n",
      "[LeakyReLU_SGD] Epoch 10, Batch 800, Loss: 2.283\n",
      "[LeakyReLU_SGD] Epoch 10, Batch 1000, Loss: 2.283\n",
      "[LeakyReLU_SGD] Epoch 10, Batch 1200, Loss: 2.282\n",
      "[LeakyReLU_SGD] Epoch 10, Batch 1400, Loss: 2.281\n",
      "LeakyReLU_SGD - Test Accuracy: 14.78%\n",
      "Starting LeakyReLU_Adam\n",
      "[LeakyReLU_Adam] Epoch 1, Batch 200, Loss: 2.109\n",
      "[LeakyReLU_Adam] Epoch 1, Batch 400, Loss: 1.832\n",
      "[LeakyReLU_Adam] Epoch 1, Batch 600, Loss: 1.754\n",
      "[LeakyReLU_Adam] Epoch 1, Batch 800, Loss: 1.672\n",
      "[LeakyReLU_Adam] Epoch 1, Batch 1000, Loss: 1.610\n",
      "[LeakyReLU_Adam] Epoch 1, Batch 1200, Loss: 1.560\n",
      "[LeakyReLU_Adam] Epoch 1, Batch 1400, Loss: 1.525\n",
      "[LeakyReLU_Adam] Epoch 2, Batch 200, Loss: 1.472\n",
      "[LeakyReLU_Adam] Epoch 2, Batch 400, Loss: 1.434\n",
      "[LeakyReLU_Adam] Epoch 2, Batch 600, Loss: 1.436\n",
      "[LeakyReLU_Adam] Epoch 2, Batch 800, Loss: 1.404\n",
      "[LeakyReLU_Adam] Epoch 2, Batch 1000, Loss: 1.390\n",
      "[LeakyReLU_Adam] Epoch 2, Batch 1200, Loss: 1.390\n",
      "[LeakyReLU_Adam] Epoch 2, Batch 1400, Loss: 1.363\n",
      "[LeakyReLU_Adam] Epoch 3, Batch 200, Loss: 1.322\n",
      "[LeakyReLU_Adam] Epoch 3, Batch 400, Loss: 1.305\n",
      "[LeakyReLU_Adam] Epoch 3, Batch 600, Loss: 1.323\n",
      "[LeakyReLU_Adam] Epoch 3, Batch 800, Loss: 1.287\n",
      "[LeakyReLU_Adam] Epoch 3, Batch 1000, Loss: 1.285\n",
      "[LeakyReLU_Adam] Epoch 3, Batch 1200, Loss: 1.280\n",
      "[LeakyReLU_Adam] Epoch 3, Batch 1400, Loss: 1.279\n",
      "[LeakyReLU_Adam] Epoch 4, Batch 200, Loss: 1.237\n",
      "[LeakyReLU_Adam] Epoch 4, Batch 400, Loss: 1.227\n",
      "[LeakyReLU_Adam] Epoch 4, Batch 600, Loss: 1.230\n",
      "[LeakyReLU_Adam] Epoch 4, Batch 800, Loss: 1.210\n",
      "[LeakyReLU_Adam] Epoch 4, Batch 1000, Loss: 1.224\n",
      "[LeakyReLU_Adam] Epoch 4, Batch 1200, Loss: 1.196\n",
      "[LeakyReLU_Adam] Epoch 4, Batch 1400, Loss: 1.198\n",
      "[LeakyReLU_Adam] Epoch 5, Batch 200, Loss: 1.175\n",
      "[LeakyReLU_Adam] Epoch 5, Batch 400, Loss: 1.187\n",
      "[LeakyReLU_Adam] Epoch 5, Batch 600, Loss: 1.200\n",
      "[LeakyReLU_Adam] Epoch 5, Batch 800, Loss: 1.165\n",
      "[LeakyReLU_Adam] Epoch 5, Batch 1000, Loss: 1.195\n",
      "[LeakyReLU_Adam] Epoch 5, Batch 1200, Loss: 1.163\n",
      "[LeakyReLU_Adam] Epoch 5, Batch 1400, Loss: 1.127\n",
      "[LeakyReLU_Adam] Epoch 6, Batch 200, Loss: 1.135\n",
      "[LeakyReLU_Adam] Epoch 6, Batch 400, Loss: 1.112\n",
      "[LeakyReLU_Adam] Epoch 6, Batch 600, Loss: 1.129\n",
      "[LeakyReLU_Adam] Epoch 6, Batch 800, Loss: 1.143\n",
      "[LeakyReLU_Adam] Epoch 6, Batch 1000, Loss: 1.127\n",
      "[LeakyReLU_Adam] Epoch 6, Batch 1200, Loss: 1.121\n",
      "[LeakyReLU_Adam] Epoch 6, Batch 1400, Loss: 1.128\n",
      "[LeakyReLU_Adam] Epoch 7, Batch 200, Loss: 1.074\n",
      "[LeakyReLU_Adam] Epoch 7, Batch 400, Loss: 1.094\n",
      "[LeakyReLU_Adam] Epoch 7, Batch 600, Loss: 1.086\n",
      "[LeakyReLU_Adam] Epoch 7, Batch 800, Loss: 1.098\n",
      "[LeakyReLU_Adam] Epoch 7, Batch 1000, Loss: 1.081\n",
      "[LeakyReLU_Adam] Epoch 7, Batch 1200, Loss: 1.079\n",
      "[LeakyReLU_Adam] Epoch 7, Batch 1400, Loss: 1.078\n",
      "[LeakyReLU_Adam] Epoch 8, Batch 200, Loss: 1.028\n",
      "[LeakyReLU_Adam] Epoch 8, Batch 400, Loss: 1.048\n",
      "[LeakyReLU_Adam] Epoch 8, Batch 600, Loss: 1.072\n",
      "[LeakyReLU_Adam] Epoch 8, Batch 800, Loss: 1.042\n",
      "[LeakyReLU_Adam] Epoch 8, Batch 1000, Loss: 1.044\n",
      "[LeakyReLU_Adam] Epoch 8, Batch 1200, Loss: 1.068\n",
      "[LeakyReLU_Adam] Epoch 8, Batch 1400, Loss: 1.045\n",
      "[LeakyReLU_Adam] Epoch 9, Batch 200, Loss: 1.016\n",
      "[LeakyReLU_Adam] Epoch 9, Batch 400, Loss: 1.006\n",
      "[LeakyReLU_Adam] Epoch 9, Batch 600, Loss: 1.041\n",
      "[LeakyReLU_Adam] Epoch 9, Batch 800, Loss: 0.997\n",
      "[LeakyReLU_Adam] Epoch 9, Batch 1000, Loss: 1.012\n",
      "[LeakyReLU_Adam] Epoch 9, Batch 1200, Loss: 1.003\n",
      "[LeakyReLU_Adam] Epoch 9, Batch 1400, Loss: 1.028\n",
      "[LeakyReLU_Adam] Epoch 10, Batch 200, Loss: 0.983\n",
      "[LeakyReLU_Adam] Epoch 10, Batch 400, Loss: 1.003\n",
      "[LeakyReLU_Adam] Epoch 10, Batch 600, Loss: 0.984\n",
      "[LeakyReLU_Adam] Epoch 10, Batch 800, Loss: 0.964\n",
      "[LeakyReLU_Adam] Epoch 10, Batch 1000, Loss: 0.967\n",
      "[LeakyReLU_Adam] Epoch 10, Batch 1200, Loss: 0.990\n",
      "[LeakyReLU_Adam] Epoch 10, Batch 1400, Loss: 0.984\n",
      "LeakyReLU_Adam - Test Accuracy: 63.65%\n",
      "Starting Tanh_SGD\n",
      "[Tanh_SGD] Epoch 1, Batch 200, Loss: 2.299\n",
      "[Tanh_SGD] Epoch 1, Batch 400, Loss: 2.297\n",
      "[Tanh_SGD] Epoch 1, Batch 600, Loss: 2.297\n",
      "[Tanh_SGD] Epoch 1, Batch 800, Loss: 2.297\n",
      "[Tanh_SGD] Epoch 1, Batch 1000, Loss: 2.293\n",
      "[Tanh_SGD] Epoch 1, Batch 1200, Loss: 2.291\n",
      "[Tanh_SGD] Epoch 1, Batch 1400, Loss: 2.289\n",
      "[Tanh_SGD] Epoch 2, Batch 200, Loss: 2.285\n",
      "[Tanh_SGD] Epoch 2, Batch 400, Loss: 2.285\n",
      "[Tanh_SGD] Epoch 2, Batch 600, Loss: 2.285\n",
      "[Tanh_SGD] Epoch 2, Batch 800, Loss: 2.282\n",
      "[Tanh_SGD] Epoch 2, Batch 1000, Loss: 2.281\n",
      "[Tanh_SGD] Epoch 2, Batch 1200, Loss: 2.279\n",
      "[Tanh_SGD] Epoch 2, Batch 1400, Loss: 2.277\n",
      "[Tanh_SGD] Epoch 3, Batch 200, Loss: 2.275\n",
      "[Tanh_SGD] Epoch 3, Batch 400, Loss: 2.274\n",
      "[Tanh_SGD] Epoch 3, Batch 600, Loss: 2.269\n",
      "[Tanh_SGD] Epoch 3, Batch 800, Loss: 2.268\n",
      "[Tanh_SGD] Epoch 3, Batch 1000, Loss: 2.266\n",
      "[Tanh_SGD] Epoch 3, Batch 1200, Loss: 2.265\n",
      "[Tanh_SGD] Epoch 3, Batch 1400, Loss: 2.263\n",
      "[Tanh_SGD] Epoch 4, Batch 200, Loss: 2.259\n",
      "[Tanh_SGD] Epoch 4, Batch 400, Loss: 2.258\n",
      "[Tanh_SGD] Epoch 4, Batch 600, Loss: 2.256\n",
      "[Tanh_SGD] Epoch 4, Batch 800, Loss: 2.252\n",
      "[Tanh_SGD] Epoch 4, Batch 1000, Loss: 2.253\n",
      "[Tanh_SGD] Epoch 4, Batch 1200, Loss: 2.250\n",
      "[Tanh_SGD] Epoch 4, Batch 1400, Loss: 2.244\n",
      "[Tanh_SGD] Epoch 5, Batch 200, Loss: 2.242\n",
      "[Tanh_SGD] Epoch 5, Batch 400, Loss: 2.241\n",
      "[Tanh_SGD] Epoch 5, Batch 600, Loss: 2.240\n",
      "[Tanh_SGD] Epoch 5, Batch 800, Loss: 2.238\n",
      "[Tanh_SGD] Epoch 5, Batch 1000, Loss: 2.235\n",
      "[Tanh_SGD] Epoch 5, Batch 1200, Loss: 2.232\n",
      "[Tanh_SGD] Epoch 5, Batch 1400, Loss: 2.231\n",
      "[Tanh_SGD] Epoch 6, Batch 200, Loss: 2.223\n",
      "[Tanh_SGD] Epoch 6, Batch 400, Loss: 2.223\n",
      "[Tanh_SGD] Epoch 6, Batch 600, Loss: 2.221\n",
      "[Tanh_SGD] Epoch 6, Batch 800, Loss: 2.220\n",
      "[Tanh_SGD] Epoch 6, Batch 1000, Loss: 2.217\n",
      "[Tanh_SGD] Epoch 6, Batch 1200, Loss: 2.215\n",
      "[Tanh_SGD] Epoch 6, Batch 1400, Loss: 2.210\n",
      "[Tanh_SGD] Epoch 7, Batch 200, Loss: 2.204\n",
      "[Tanh_SGD] Epoch 7, Batch 400, Loss: 2.201\n",
      "[Tanh_SGD] Epoch 7, Batch 600, Loss: 2.202\n",
      "[Tanh_SGD] Epoch 7, Batch 800, Loss: 2.202\n",
      "[Tanh_SGD] Epoch 7, Batch 1000, Loss: 2.195\n",
      "[Tanh_SGD] Epoch 7, Batch 1200, Loss: 2.191\n",
      "[Tanh_SGD] Epoch 7, Batch 1400, Loss: 2.192\n",
      "[Tanh_SGD] Epoch 8, Batch 200, Loss: 2.184\n",
      "[Tanh_SGD] Epoch 8, Batch 400, Loss: 2.184\n",
      "[Tanh_SGD] Epoch 8, Batch 600, Loss: 2.178\n",
      "[Tanh_SGD] Epoch 8, Batch 800, Loss: 2.179\n",
      "[Tanh_SGD] Epoch 8, Batch 1000, Loss: 2.180\n",
      "[Tanh_SGD] Epoch 8, Batch 1200, Loss: 2.171\n",
      "[Tanh_SGD] Epoch 8, Batch 1400, Loss: 2.171\n",
      "[Tanh_SGD] Epoch 9, Batch 200, Loss: 2.169\n",
      "[Tanh_SGD] Epoch 9, Batch 400, Loss: 2.164\n",
      "[Tanh_SGD] Epoch 9, Batch 600, Loss: 2.158\n",
      "[Tanh_SGD] Epoch 9, Batch 800, Loss: 2.168\n",
      "[Tanh_SGD] Epoch 9, Batch 1000, Loss: 2.152\n",
      "[Tanh_SGD] Epoch 9, Batch 1200, Loss: 2.156\n",
      "[Tanh_SGD] Epoch 9, Batch 1400, Loss: 2.150\n",
      "[Tanh_SGD] Epoch 10, Batch 200, Loss: 2.146\n",
      "[Tanh_SGD] Epoch 10, Batch 400, Loss: 2.149\n",
      "[Tanh_SGD] Epoch 10, Batch 600, Loss: 2.144\n",
      "[Tanh_SGD] Epoch 10, Batch 800, Loss: 2.139\n",
      "[Tanh_SGD] Epoch 10, Batch 1000, Loss: 2.137\n",
      "[Tanh_SGD] Epoch 10, Batch 1200, Loss: 2.131\n",
      "[Tanh_SGD] Epoch 10, Batch 1400, Loss: 2.137\n",
      "Tanh_SGD - Test Accuracy: 25.22%\n",
      "Starting Tanh_Adam\n",
      "[Tanh_Adam] Epoch 1, Batch 200, Loss: 2.035\n",
      "[Tanh_Adam] Epoch 1, Batch 400, Loss: 1.784\n",
      "[Tanh_Adam] Epoch 1, Batch 600, Loss: 1.689\n",
      "[Tanh_Adam] Epoch 1, Batch 800, Loss: 1.634\n",
      "[Tanh_Adam] Epoch 1, Batch 1000, Loss: 1.582\n",
      "[Tanh_Adam] Epoch 1, Batch 1200, Loss: 1.546\n",
      "[Tanh_Adam] Epoch 1, Batch 1400, Loss: 1.488\n",
      "[Tanh_Adam] Epoch 2, Batch 200, Loss: 1.458\n",
      "[Tanh_Adam] Epoch 2, Batch 400, Loss: 1.450\n",
      "[Tanh_Adam] Epoch 2, Batch 600, Loss: 1.419\n",
      "[Tanh_Adam] Epoch 2, Batch 800, Loss: 1.398\n",
      "[Tanh_Adam] Epoch 2, Batch 1000, Loss: 1.385\n",
      "[Tanh_Adam] Epoch 2, Batch 1200, Loss: 1.373\n",
      "[Tanh_Adam] Epoch 2, Batch 1400, Loss: 1.348\n",
      "[Tanh_Adam] Epoch 3, Batch 200, Loss: 1.292\n",
      "[Tanh_Adam] Epoch 3, Batch 400, Loss: 1.335\n",
      "[Tanh_Adam] Epoch 3, Batch 600, Loss: 1.271\n",
      "[Tanh_Adam] Epoch 3, Batch 800, Loss: 1.267\n",
      "[Tanh_Adam] Epoch 3, Batch 1000, Loss: 1.267\n",
      "[Tanh_Adam] Epoch 3, Batch 1200, Loss: 1.285\n",
      "[Tanh_Adam] Epoch 3, Batch 1400, Loss: 1.242\n",
      "[Tanh_Adam] Epoch 4, Batch 200, Loss: 1.222\n",
      "[Tanh_Adam] Epoch 4, Batch 400, Loss: 1.209\n",
      "[Tanh_Adam] Epoch 4, Batch 600, Loss: 1.196\n",
      "[Tanh_Adam] Epoch 4, Batch 800, Loss: 1.205\n",
      "[Tanh_Adam] Epoch 4, Batch 1000, Loss: 1.200\n",
      "[Tanh_Adam] Epoch 4, Batch 1200, Loss: 1.167\n",
      "[Tanh_Adam] Epoch 4, Batch 1400, Loss: 1.188\n",
      "[Tanh_Adam] Epoch 5, Batch 200, Loss: 1.142\n",
      "[Tanh_Adam] Epoch 5, Batch 400, Loss: 1.155\n",
      "[Tanh_Adam] Epoch 5, Batch 600, Loss: 1.140\n",
      "[Tanh_Adam] Epoch 5, Batch 800, Loss: 1.128\n",
      "[Tanh_Adam] Epoch 5, Batch 1000, Loss: 1.141\n",
      "[Tanh_Adam] Epoch 5, Batch 1200, Loss: 1.122\n",
      "[Tanh_Adam] Epoch 5, Batch 1400, Loss: 1.130\n",
      "[Tanh_Adam] Epoch 6, Batch 200, Loss: 1.088\n",
      "[Tanh_Adam] Epoch 6, Batch 400, Loss: 1.082\n",
      "[Tanh_Adam] Epoch 6, Batch 600, Loss: 1.097\n",
      "[Tanh_Adam] Epoch 6, Batch 800, Loss: 1.067\n",
      "[Tanh_Adam] Epoch 6, Batch 1000, Loss: 1.088\n",
      "[Tanh_Adam] Epoch 6, Batch 1200, Loss: 1.082\n",
      "[Tanh_Adam] Epoch 6, Batch 1400, Loss: 1.083\n",
      "[Tanh_Adam] Epoch 7, Batch 200, Loss: 1.053\n",
      "[Tanh_Adam] Epoch 7, Batch 400, Loss: 1.044\n",
      "[Tanh_Adam] Epoch 7, Batch 600, Loss: 1.054\n",
      "[Tanh_Adam] Epoch 7, Batch 800, Loss: 1.037\n",
      "[Tanh_Adam] Epoch 7, Batch 1000, Loss: 1.008\n",
      "[Tanh_Adam] Epoch 7, Batch 1200, Loss: 1.020\n",
      "[Tanh_Adam] Epoch 7, Batch 1400, Loss: 1.022\n",
      "[Tanh_Adam] Epoch 8, Batch 200, Loss: 1.003\n",
      "[Tanh_Adam] Epoch 8, Batch 400, Loss: 1.000\n",
      "[Tanh_Adam] Epoch 8, Batch 600, Loss: 1.006\n",
      "[Tanh_Adam] Epoch 8, Batch 800, Loss: 0.999\n",
      "[Tanh_Adam] Epoch 8, Batch 1000, Loss: 0.984\n",
      "[Tanh_Adam] Epoch 8, Batch 1200, Loss: 0.995\n",
      "[Tanh_Adam] Epoch 8, Batch 1400, Loss: 0.990\n",
      "[Tanh_Adam] Epoch 9, Batch 200, Loss: 0.965\n",
      "[Tanh_Adam] Epoch 9, Batch 400, Loss: 0.958\n",
      "[Tanh_Adam] Epoch 9, Batch 600, Loss: 0.978\n",
      "[Tanh_Adam] Epoch 9, Batch 800, Loss: 0.960\n",
      "[Tanh_Adam] Epoch 9, Batch 1000, Loss: 0.959\n",
      "[Tanh_Adam] Epoch 9, Batch 1200, Loss: 0.958\n",
      "[Tanh_Adam] Epoch 9, Batch 1400, Loss: 0.942\n",
      "[Tanh_Adam] Epoch 10, Batch 200, Loss: 0.933\n",
      "[Tanh_Adam] Epoch 10, Batch 400, Loss: 0.929\n",
      "[Tanh_Adam] Epoch 10, Batch 600, Loss: 0.948\n",
      "[Tanh_Adam] Epoch 10, Batch 800, Loss: 0.905\n",
      "[Tanh_Adam] Epoch 10, Batch 1000, Loss: 0.923\n",
      "[Tanh_Adam] Epoch 10, Batch 1200, Loss: 0.928\n",
      "[Tanh_Adam] Epoch 10, Batch 1400, Loss: 0.909\n",
      "Tanh_Adam - Test Accuracy: 64.76%\n",
      "All experiments completed. Launch TensorBoard with: tensorboard --logdir=runs\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    activations = {\n",
    "        \"LeakyReLU\": nn.LeakyReLU(negative_slope=0.1),\n",
    "        \"Tanh\": nn.Tanh()\n",
    "    }\n",
    "    optimizers = {\n",
    "        \"SGD\": lambda params: optim.SGD(params, lr=0.0001),\n",
    "        \"Adam\": lambda params: optim.Adam(params, lr=0.0001)\n",
    "    }\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for act_name, activation in activations.items():\n",
    "        for opt_name, opt_fn in optimizers.items():\n",
    "            experiment_name = f\"{act_name}_{opt_name}\"\n",
    "            writer = SummaryWriter(log_dir=f\"runs/{experiment_name}\")\n",
    "            model = CNN(activation=activation).to(device)\n",
    "            optimizer = opt_fn(model.parameters())\n",
    "            train_and_evaluate(model, train_loader, test_loader, optimizer, criterion, device, writer, experiment_name)\n",
    "            writer.close()\n",
    "\n",
    "    print(\"All experiments completed\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 — Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.2.1 — Transfer Learning from ImageNet\n",
    "Using AlexNet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.2.2 — Transfer Learning from MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
