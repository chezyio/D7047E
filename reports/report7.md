# Report 7

Describe what you have learned during this study period. You can also include references to external material you have used to enhance your learning process (e.g., book chapters, scientific papers, online courses, online sites).

| Task     | Time Spent (hours) |
| -------- | ------------------ |
| Lectures | 2                  |
| Labs     | 4                  |
| Project  | 6                  |

## Knowledge Distillation and Quantization
I have deepened my understanding of several advanced AI concepts that enhance the efficiency, adaptability, and scalability of systems. Knowledge Distillation introduces the teacher-student training framework, where a smaller, lightweight student model learns to mimic the behavior of a larger teacher model. By matching both true labels and the teacherâ€™s soft predictions, the student can capture nuanced patterns while maintaining efficiency. Similarly, Quantization focuses on reducing model precision, such as using 8-bit integers instead of 32-bit floating points, to create smaller, faster models. I explored various strategies like Post-training Quantization (PTQ) for applying quantization after training and Quantization-aware Training (QAT), which incorporates it during training. I also learned about different quantization types, including weight-only, full quantization, dynamic range, and integer-only approaches.

## Artificial Curiosity, AutoML and Meta Learning
The concept of Artificial Curiosity stood out for its human-like approach to learning, driven by intrinsic motivation rather than extrinsic rewards. Intrinsic rewards, such as novelty, surprise, and prediction error, enable AI to balance exploration with exploitation and maximize information gain, often modeled through Bayesian surprise. In parallel, AutoML automates the end-to-end machine learning pipeline, making it accessible to non-experts by streamlining tasks like hyperparameter tuning, model selection, and deployment. Techniques like Bayesian optimization, genetic algorithms, reinforcement learning (RL), and gradient-based methods optimize the process. Finally, Meta-learning emphasizes learning to adapt efficiently to new tasks with minimal data. By leveraging experiences from related tasks, models can generalize quickly, employing approaches such as metric-based, model-based, and optimization-based strategies. Together, these concepts highlight the innovative ways AI systems can be designed to be efficient, adaptive, and accessible.

## Project
We have concluded our findings and currently working on the slides to prepare for presentation.