# Report 6

Describe what you have learned during this study period. You can also include references to external material you have used to enhance your learning process (e.g., book chapters, scientific papers, online courses, online sites).

| Task     | Time Spent (hours) |
| -------- | ------------------ |
| Lectures | 4                  |
| Labs     | 10                 |
| Project  | 5                  |

## Reinforcement Learning

I’ve come to appreciate how different it is from other forms of machine learning like supervised or unsupervised learning. Unlike those methods, RL doesn’t rely on labeled datasets or clustering algorithms. Instead, it thrives on a reward signal, which acts as a delayed form of feedback, guiding the agent’s behavior over time. What’s fascinating is how time and causality play a crucial role here—each action not only affects the current outcome but also shapes the data the agent receives in the future.

The concept of maximizing cumulative rewards resonates deeply with me. It frames the agent’s purpose in a simple yet profound way: every decision made should ultimately work toward a long-term goal. The reward hypothesis, which suggests all objectives can be boiled down to this maximization, ties everything together in an elegant framework.

Understanding Markov states was another eye-opener. I learned that a state is Markovian if it contains all the necessary information from the past to predict the future, making the process inherently efficient. This Markov property simplifies complex systems by focusing only on the present state without needing to carry the weight of the entire history.

I also gained insights into the challenges agents face in environments that are not fully observable. In such cases, the agent must navigate uncertainties and infer the best course of action from incomplete data, adding an intriguing layer of complexity.

The major components of an RL agent—policy, value function, and model—offer a structured way to think about decision-making. I found it especially interesting how policies can be deterministic or stochastic, depending on whether actions are fixed or probabilistic. Additionally, value functions and models emphasize foresight, teaching agents to not just act but also predict and adapt.

Finally, exploring different types of RL agents, such as value-based, policy-based, and actor-critic, broadened my understanding of how diverse strategies can be employed depending on the problem at hand. Each approach comes with its strengths and trade-offs, reflecting the richness of RL as a field.

## TinyML

I’ve also gained a deeper appreciation for how machine learning can be adapted to fit the constraints of small, low-power devices. TinyML is not just a technological advancement but also a demonstration of how AI can be democratized, making powerful capabilities accessible on devices with limited resources. It has been fascinating to see how these tiny models enable real-world applications like smart doorbells and fitness trackers, blending intelligence with practicality in everyday tools.

## Project

I've have dedicated most of my time to look at various inference techniques that can help to detect small objects, SAHI in particular.
