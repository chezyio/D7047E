# Report 2

Describe what you have learned during this study period. You can also include references to external material you have used to enhance your learning process (e.g., book chapters, scientific papers, online courses, online sites).

| Task     | Time Spent (hours) |
| -------- | ------------------ |
| Lectures | 4                  |
| Labs     | 6                  |

## Generative Modelling

This week, I delved into foundational concepts related to probability distribution learning and density estimation. The primary goal was to understand how to train models that can represent the underlying probability distribution of a given dataset. These models allow the generation of new data instances that resemble the original data by effectively capturing the probabilistic structure of the dataset. The focus was on techniques that model this distribution explicitly, offering insights into how data behaves and can be represented mathematically.

I explored latent variable models, particularly their role in learning compressed, interpretable representations of data. These models include Autoencoders, Variational Autoencoders (VAEs), and Generative Adversarial Networks (GANs). Autoencoders, an unsupervised approach, learn lower-dimensional representations by compressing data into a latent vector and reconstructing the input data using a decoder. A key component is the bottleneck hidden layer, which forces the network to capture compact and meaningful latent representations. However, due to their deterministic nature, Autoencoders often reconstruct the same output for the same input. To address this limitation, VAEs introduce stochasticity and probabilistic priors, such as a Gaussian distribution, to improve latent space learning. VAEs regularize the encoding process, ensuring latent representations are evenly distributed and discouraging clustering in specific regions. This regularization introduces both continuity (similar latent points decode into similar outputs) and completeness (samples from latent space produce meaningful outputs). Additionally, latent perturbation, where individual latent variables are varied while keeping others fixed, highlighted how different latent dimensions encode specific, interpretable features.

Moving to Generative Adversarial Networks (GANs), I studied their ability to generate realistic samples by directly transforming low-dimensional noise vectors into complex data representations. GANs consist of two competing networks: a Generator, which attempts to create data that mimics the real dataset, and a Discriminator, which differentiates between real and fake data. Unlike models that explicitly model data density, GANs rely on sampling and transformations to match the data distribution. Training involves a competition between the Generator and Discriminator, gradually improving the Generator’s ability to produce realistic samples. GANs stand in contrast to diffusion models, which generate samples iteratively by refining noise over time.

## Multi-modal Learning

Lastly, I explored multi-modal learning, an area focused on integrating data from different modalities such as text, images, audio, and video. Multi-modal models rely on specialized embeddings and fusion techniques to create unified data representations that capture relationships across modalities. Each modality undergoes preprocessing to generate embeddings—examples include Word2Vec for text, CNNs for images, and numerical representations for structured data. These embeddings are then fused to produce a final multi-modal representation. Fusion can occur at different stages: early fusion, where modalities are combined early in the pipeline; intermediate fusion, which concatenates feature representations before prediction; and late fusion, which combines outputs of independently processed modalities. State-of-the-art architectures like Data2Vec and VilBERT demonstrate the potential of these techniques in handling multi-modal data effectively. This week’s learning emphasized the importance of embedding generation and fusion strategies in enabling AI models to understand and process complex, multi-modal inputs.
